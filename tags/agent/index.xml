<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Agent on 元视角</title><link>https://blog.yuanpei.me/tags/agent/</link><description>Recent content in Agent on 元视角</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Sun, 24 Aug 2025 20:42:23 +0000</lastBuildDate><atom:link href="https://blog.yuanpei.me/tags/agent/index.xml" rel="self" type="application/rss+xml"/><item><title>基于 Supabase 的 AI 应用开发探索</title><link>https://blog.yuanpei.me/posts/supabase-powered-ai-app-exploration/</link><pubDate>Sun, 24 Aug 2025 20:42:23 +0000</pubDate><guid>https://blog.yuanpei.me/posts/supabase-powered-ai-app-exploration/</guid><description>引言 最近看到一个有趣的观点：当你试图对 AI 进行某种界定时，会发现诸如知识密集型、资本密集型、劳动密集型、资源密集型这些类别，似乎都适用于它。首先，AI 无疑是知识密集型和资本密集型的。然而，考虑到数据标注等工作需要投入大量人力，AI 表现出劳动密集型的特征。与此同时，AI 的训练和推理依赖庞大的算力，而这背后离不开巨大的资源支持，所以它还具备资源密集型的属性。这种“什么都是，又什么都不是”、难以简单归类的状态，恰恰构成了 AI 产业最根本的特性。在持续使用 Claude Code 两个月以后，一切都归于寂静，曾经的 Cursor 和 Windsurf 亦是如此。秋风渐起，天气转凉，最近发布的 GPT-5 和 DeepSeek-v3.1 表现不温不火，反倒是谷歌凭借 Nano Banana 再次成为焦点。当然，相比于讨论这些无关紧要的事情，我更关注 AI 技术在实际场景中的落地。因此，在这篇博客中，我想和大家分享如何基于 Supabase 快速构建一个可用的 AI 应用。
为什么选择 Supabase? 时间来到 2025 年，横亘在我们面前的最大危机，已从「怎么做」变成「做什么」。Know-How 里的 Know 与 How，正在逐渐被 AI 接管，留给我们的只剩下 What——我们究竟该让技术指向何方。放眼望去，市面上可供开发 AI 应用的工具琳琅满目，令人目不暇接。以笔者为例，Cursor、Windsurf、Cline、Claude Code、Gemini CLI，各种工具几乎都尝试了一遍，而如今最常用的，反而是 VSCode 内置的 GitHub Copilot。结合我有限的认知，我对这些工具做了如下划分，大家可以按图索骥，选择适合自己的工具进行尝试：
编辑器/IDE/插件类：Cursor、Windsurf、Cline、GitHub Copilot 等 CLI/工具类： Claude Code、Gemini CLI 等 一站式部署类：Bolt.New、v0 等 低代码/工作流：Coze、Dify、n8n 等 代码框架：LangGraph、Semantic Kernel、AutoGen、CrewAI 等 那么，相对于这些这些方案，Supabase 有什么优势呢？开发 AI 应用时，我们真正需要的，是一个既能满足 AI 应用需求，又能快速开发和部署的平台。而 Supabase 正是这样一个理想的平台，它具有以下优势：</description></item><item><title>四点钟海棠花未眠</title><link>https://blog.yuanpei.me/posts/four-o-clock-hibiscus-awake/</link><pubDate>Fri, 18 Apr 2025 13:12:22 +0000</pubDate><guid>https://blog.yuanpei.me/posts/four-o-clock-hibiscus-awake/</guid><description>无论你内心期盼过多少次的春和景明，夏天终归如约而至，春天则不情愿地悄然谢幕。虽未至立夏，可空调、风扇的争相开启，无一不在昭示着春天的形同虚设。西安的春秋季节向来短暂，一场清明时节的沙尘暴，便轻易地带走了我们对春天的全部幻想。踏青赏花、旅行露营，世间美好与你环环相扣。遥想当年，魏晋风流人物，于兰亭集会，引以为流觞曲水，想来不会比此刻更添雅致。可惜，“向之所欣，俯仰之间，已为陈迹”，今人与古人的感慨，竟是如此的不谋而合。第一次读到川端康成那句“凌晨四点钟，看到海棠花未眠”，令我心动的是这个意象中蕴藏着的深邃美感。然而，当我有一天站在真正的海棠花前却认不出它时，我终于意识到自己的浅薄。在一个同样无眠的夜晚，我决定写一篇文章，借以描摹我此刻的心境，以及这一个月以来关于 AI 协同、日本战国历史、 Agent 设计的种种心绪。
新主题 如你所见，我给博客更换了一个新主题，该主题移植自 imsyy 的 vitepress-theme-curve。至此，使用了三年多的 hugo-theme-stack 主题正式宣告退役。当然，我想表达的是，这个移植工作是由我和 Cursor 一起完成的。作为一名后端工程师，当我从 Hexo 切换到 Hugo 时，我曾经做过类似的尝试，可无一例外均以失败告终。所以，除了惊叹于 AI 带来的这种生产力的放大，我实在想不出还有什么更令人叹服的东西，以一言蔽之，接受 AI 比自己强大。
新版博客主题 有趣的地方在于，我前面三次尝试让 Cursor 完成这个任务，收获到的是不同程度的失败。例如，移植后主题运行报错、移植后主题没有遵从原主题的结构或样式等，哪怕我使用的是 Cursor 的 Agent 模式。直到第四次的时候，一切开始步入正轨。最终，它变成了你现在看到的这个新主题。如果说第四次尝试有什么魔法的话，那一定是我的脑海中有了一个更详细的移植计划，我开始主导我和 AI 整个会话过程，而不是像一个新手或者小白一样狂按 Tab 键。
Git 见证了主题的移植过程 首先，一个博客至少需要两种布局，即：文章列表、文章详情。基于这个认知，我开始尝试将原主题中的 Vue 组件与 Hugo 中的 list.html 以及 single.html 对应起来。这种对应并非简单地将其添加到 Cursor 的上下文中，而是提前建好基本的目录结构、同时在提示词里面写清楚相对路径。对于页面中的子组件，我允许 Cursor 先创建一个空白的文件充当占位符。其次，在和 AI 互动的过程中，对于可工作的产物需要及时提交到 Git 中，从而避免程序被 AI 改坏。我认为，这是一个非常重要的技巧，敏捷开发中“可工作的软件”这一理念，得以在这一刻被具象化。
全新的书音影页面 如今，LLM 和人类的注意力都非常宝贵，在和 Cursor 交互的过程中有意识地控制上下文的范围非常重要。正如我们人与人之间交流，上下文并不是越多越好，任何时候你都应该注意，不向对方提供对方不需要的上下文。诚然。Cursor 中的 Codebase 可以自动索引不同的文件，可如果你有机会缩小检索范围，何乐而不为呢？难道大海捞针是一件值得推崇的事情吗？在移植了主要的布局、样式以后，我开始逐个移植其中的子组件，于我个人而言，这其实是一堂面向大模型的沟通课，相比于人类的言不由衷，同大模型交流需要的是足够的真诚，可对人类而言这恰恰是一种稀缺资源。雷总说，“真诚是必杀技”，可我们好像更喜欢不好好说话。情绪价值充满字里行间的时候，信息密度就会低到令人发指，低幼化的语言表达，犹如微小剂量的砷，无形中变成了我们和 LLM 之间的沟通障碍。
15. 言语犹如微小剂量的砷 随着模型能力的不断升级，现在人们好像随便输入一通，就能得到非常不错的结果。于是，有人开始说，提示工程不存在了，甚至连 Agent 都不存在了。可我始终相信一个经典的理论：Garbage In，Garbage Out，向对方清楚地表达出你的意愿，这并不是什么高标准，而是一个基本要求。可惜，在人类世界里待久了，有喜欢揣摩潜台词的人，就有喜欢模棱两可的人。我始终认为，人类和 AI 在未来应该是一种平等的、合作的关系，我们不该在这种关系中代入类似上下级、老板/员工这样的不对等的关系。要求别人或者 AI 正确地揣摩你的意图，这并不是 NLP 技术或者情感陪伴类产品中的刚需，而是一种隐形的情绪霸凌，因为你本可以选择更清晰的表达方式，可你偏偏没有这样做。</description></item><item><title>Semantic Kernel × MCP：智能体的上下文增强探索</title><link>https://blog.yuanpei.me/posts/semantic-kernel-mcp-agent-context-enhanced-exploration/</link><pubDate>Sun, 09 Mar 2025 20:42:23 +0000</pubDate><guid>https://blog.yuanpei.me/posts/semantic-kernel-mcp-agent-context-enhanced-exploration/</guid><description>时光飞逝，转眼间已步入阳春三月，可我却迟迟未曾动笔写下 2025 年的第一篇 AI 博客。不知大家心中作何感想，从年初 DeepSeek 的爆火出圈，到近期 Manus 的刷屏热议，AI 领域的发展可谓是日新月异。例如，DeepSeek R1 的出现，让人们开始接受慢思考，可我们同样注意到，OpenAI 的 Deep Research 选择了一条和 R1 截然不同的路线，模型与智能体之间的界限开始变得模糊。对于这一点，使用过 Cursor Composer 或者 Deep Research 的朋友，相信你们会有更深刻的感悟。有人说，Agent 会成为 2025 年的 AI 主旋律。我不知道大家是否清楚 AutoGPT 与 Manus 的差别，对我个人而言，最重要的事情是在喧嚣过后找到 “值得亲手去做的事情”。所以，今天这篇博客，我想分享一个 “熟悉而陌生” 的东西：MCP，即：模型上下文协议，并尝试将这个协议和 Semantic Kernel 连接起来。
MCP 介绍 [TL;DR] MCP 是由 Anthropic 设计的开放协议，其定位类似于 AI 领域的 USB 接口，旨在通过统一接口解决大模型连接不同数据源和工具的问题。该协议通过 JSON-RPC 规范定义了 Prompt 管理、资源访问和工具调用三大核心能力，使得任何支持 Function Calling 的模型都能无缝对接外部系统，从而帮助大语言模型实现 “万物互联”。
什么是 MCP? MCP（Model Context Protocol）是由 Anthropic 设计的一种开放协议，旨在标准化应用程序向大语言模型（LLMs）提供上下文的方式，使大模型能够以统一的方法连接各种数据源和工具。你可以将其理解为 AI 应用的 USB 接口，为 AI 模型连接到不同的数据源和工具提供了标准化的方法。架构设计上，MCP 采用了经典的 C/S 架构，客户端可以使用该协议灵活地连接多个 MCP Server，从而获取丰富的数据和功能支持，如下图所示：</description></item><item><title>Semantic Kernel 视角下的 Text2SQL 实践与思考</title><link>https://blog.yuanpei.me/posts/semantic-kernel-driven-text2sql-practice/</link><pubDate>Mon, 15 Jul 2024 20:42:23 +0000</pubDate><guid>https://blog.yuanpei.me/posts/semantic-kernel-driven-text2sql-practice/</guid><description>《诗经》有言：七月流火，九月授衣，这句话常被用来描绘夏秋交替、天气由热转凉的季节变化。西安的雨季，自六月下旬悄然而至、连绵不绝，不由地令人感慨：古人诚不欺我。或许，七月注定是个多事之“秋”，前有萝卜快跑及其背后的无人驾驶引发热议，后有特朗普在宾夕法尼亚州竞选集会时遇刺，更遑论洞庭湖决口、西二环塌方。杨绛先生说，成长就是学会心平气和地去面对这世界的兵荒马乱，可真正的战争“俄乌冲突”至今已经持续800多天。有时候，我不免怀疑，历史可是被诅咒了的时间？两年前的此时此刻，日本前首相安倍晋三遇刺身亡，我专门写过一篇文章《杂感·七月寄望》 。现在，回想起两人长达19秒的史诗级握手画面，一时间居然有种“一笑泯恩仇”的错觉。因为，从某种意义上来说，他们似乎成为了共患难的“战友”。雍正之于万历，如同特朗普之于肯尼迪，虽时过境迁，而又似曾相识，大概世间万物总逃不出某种循环。最近一个月，从 RAG 到 Agent，再到微软 GraphRAG 的爆火，诸如 Graph、NER、知识图谱等知识点再次被激活。我突然觉得，我需要一篇文章来整理我当下的思绪。
实现 Agent 以后 参照复旦大学的 RAG 综述论文实现 Advance RAG 以后，我开始将目标转向 Agent。一般来说，一个 Agent 至少应该具备三种基本能力：规划(Planning)、记忆(Memory)以及工具使用(Tool Use)，即：Agent = LLM + Planning + Memory + Tool Use。如果说，使用工具是人类文明的起点，那么，Agent 则标志着大模型从 “说话” 进化到 “做事”。目前的 Agent 或者是说智能体，本质上都是将大模型视作数字大脑，通过反思、CoT、ReAct 等提示工程模拟人类思考过程，再通过任务规划、工具使用来扩展其能力的边界，使其能够感知和连接真实世界。从早期的 AutoGPT 到全球首个 AI 程序员智能体 Devin，人们对于 AI 的期望值，正肉眼可见地一路水涨船高。
Agent 的基本概念 目前，市场上主流新能源汽车的智驾系统都大多处于 L2 或 L3 级别，萝卜快跑则率迈进 L4 级别。尽管我可以理解这一发展趋势的必然性，可当我意识到碳基生命自身的偶然性，我想知道，那些可能导致成千上万的人失业的失业的科技创新，是否是显得过于残酷和冰冷？在2024年的上半年，我接触到了多种 Agent 产品，例如 FastGPT、Coze、Dify 等等。这些产品基本都是基于工作流编排的思路，这实际上是一种对大型模型不稳定输出和多轮对话调用成本的妥协。受到过往工作经历影响，我对于工作流和低代码非常反感。因此，我坚信大模型动态地规划和执行任务的能力才是未来。在实现 Agent 的过程中，我参考 Semantic Kernel 的一个 PR 实现了一个支持 ReAct 模式的 Planner，这证明了我从去年开始接触大型模型时的种种想法，到目前为止基本上都是正确的。
当下生成式 AI 的优化方向 我主张采用小模型结合插件的方式，推进 AI 服务的本地化，因为一味地追求参数规模或上下文长度，只会陷入永无休止的百模大战。在技术和成本之间，你必须要找到一个平衡点。例如，最近大火的 GraphRAG，知识图谱结合大模型的理念虽好，但构建知识图谱的成本相对较高，运行一个简单示例的费用大约在5到10美元左右。在实现 Agent 的过程中，我发现，使用阿里的 Qwen2-7B 模型完全可以支持任务规划以及参数提取，唯一的问题是 Ollama 推理速度较慢，尤其是在纯 CPU 推理的情况下。此外，目前的 Agent 的反思功能大多依赖于多轮对话，其效果易受上下文长度的影响。即便使用 OpenAI、Moonshot 等厂商的服务，它们的 TPM/RPM 通常不会太高，导致公共 API 难以满足 Agent 的运行需求。如果增加接口调用间隔，无疑又会让屏幕前的用户失去耐心。因此，即便是在 token 价格越来越便宜的情况下，以任务为导向的 Agent，其 token 消耗量依然是一笔不小的开销。</description></item></channel></rss>