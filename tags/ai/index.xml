<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI on 元视角</title><link>https://blog.yuanpei.me/tags/ai/</link><description>Recent content in AI on 元视角</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Tue, 13 Jan 2026 22:00:00 +0000</lastBuildDate><atom:link href="https://blog.yuanpei.me/tags/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>当我用 2000 条聊天记录，让 AI 为我画一幅自画像</title><link>https://blog.yuanpei.me/posts/ai-mirror-self-analysis/</link><pubDate>Tue, 13 Jan 2026 22:00:00 +0000</pubDate><guid>https://blog.yuanpei.me/posts/ai-mirror-self-analysis/</guid><description>或许，最了解我们的，不是自己，而是我们说过的话。
起因 大概是年前那段时间，我脑子里反复盘旋着一个问题：
我和 AI 聊了这么多，它们会不会比我自己更了解我？
这个念头一旦冒出来，就像一颗种子，在我脑海里生根发芽。我手里有三个平台的聊天记录——DeepSeek、ChatGPT 和 Kimi，加起来一共 2050 条对话，4394 条消息。如果把这些数据完整交给另一个 AI，让它从第三视角分析我，会得到怎样的结论？坦白讲，我并不期待什么石破天惊的发现，更多只是好奇：当所有零散的对话被汇总、被量化、被抽象之后，那个被 AI 描绘出来的「我」，会不会和我以为的自己不太一样。
数据概览 第一次分析：DeepSeek 的初见 一开始，我先让 AI 单独分析了 DeepSeek 的记录。这是我最常用的代码助手，我原本以为它会直接给我贴上「技术宅」的标签。但是，结果比我想象中具体得多：
上午 9-11 点最活跃 51% 的对话是「短平快」型（3 句话以内结束） 高频请求是「写代码」「解释原理」 关键词集中在 Python、SQL、AI、机器学习 AI 给出的总结是：「你是一个追求效率、喜欢直接要结果的开发者。」
年度关键词 活跃度热力图 我看完愣了几秒，然后不自觉地笑了，好像&amp;hellip;&amp;hellip;确实是这样。
第二次分析：三个平台，三个「我」 当我把三个平台的数据放在一起对比时，发现了一件非常有意思的事。同一个我，在不同工具上，竟然表现出完全不同的状态：
平台 对话特征 我的状态 DeepSeek 51% 短对话 执行模式：快速出活 ChatGPT 36% 长对话 思考模式：深入探讨 Kimi 100% 极短对话 闲聊模式：翻译、润色 这感觉有点像工作中的角色切换——面对不同的人，我们会自然调整说话方式:
DeepSeek 更像「我的手」，负责把想法快速落地 ChatGPT 更像「我的大脑」，陪我拆解问题、反复推演 Kimi 则像「速记本」，处理零碎但必须完成的小事 意识到这一点时，我突然明白了一件事：工具不只是被使用的对象，它同样在反向塑造我们的行为模式。
不同平台的 “我” 第三次分析：AI 眼中的「我」 当 AI 整合了所有数据后，给出了几段让我印象深刻的描述：</description></item><item><title>基于 Supabase 的 AI 应用开发探索</title><link>https://blog.yuanpei.me/posts/supabase-powered-ai-app-exploration/</link><pubDate>Sun, 24 Aug 2025 20:42:23 +0000</pubDate><guid>https://blog.yuanpei.me/posts/supabase-powered-ai-app-exploration/</guid><description>引言 最近看到一个有趣的观点：当你试图对 AI 进行某种界定时，会发现诸如知识密集型、资本密集型、劳动密集型、资源密集型这些类别，似乎都适用于它。首先，AI 无疑是知识密集型和资本密集型的。然而，考虑到数据标注等工作需要投入大量人力，AI 表现出劳动密集型的特征。与此同时，AI 的训练和推理依赖庞大的算力，而这背后离不开巨大的资源支持，所以它还具备资源密集型的属性。这种“什么都是，又什么都不是”、难以简单归类的状态，恰恰构成了 AI 产业最根本的特性。在持续使用 Claude Code 两个月以后，一切都归于寂静，曾经的 Cursor 和 Windsurf 亦是如此。秋风渐起，天气转凉，最近发布的 GPT-5 和 DeepSeek-v3.1 表现不温不火，反倒是谷歌凭借 Nano Banana 再次成为焦点。当然，相比于讨论这些无关紧要的事情，我更关注 AI 技术在实际场景中的落地。因此，在这篇博客中，我想和大家分享如何基于 Supabase 快速构建一个可用的 AI 应用。
为什么选择 Supabase? 时间来到 2025 年，横亘在我们面前的最大危机，已从「怎么做」变成「做什么」。Know-How 里的 Know 与 How，正在逐渐被 AI 接管，留给我们的只剩下 What——我们究竟该让技术指向何方。放眼望去，市面上可供开发 AI 应用的工具琳琅满目，令人目不暇接。以笔者为例，Cursor、Windsurf、Cline、Claude Code、Gemini CLI，各种工具几乎都尝试了一遍，而如今最常用的，反而是 VSCode 内置的 GitHub Copilot。结合我有限的认知，我对这些工具做了如下划分，大家可以按图索骥，选择适合自己的工具进行尝试：
编辑器/IDE/插件类：Cursor、Windsurf、Cline、GitHub Copilot 等 CLI/工具类： Claude Code、Gemini CLI 等 一站式部署类：Bolt.New、v0 等 低代码/工作流：Coze、Dify、n8n 等 代码框架：LangGraph、Semantic Kernel、AutoGen、CrewAI 等 那么，相对于这些这些方案，Supabase 有什么优势呢？开发 AI 应用时，我们真正需要的，是一个既能满足 AI 应用需求，又能快速开发和部署的平台。而 Supabase 正是这样一个理想的平台，它具有以下优势：</description></item><item><title>微博 × MCP：社交媒体新玩法解锁</title><link>https://blog.yuanpei.me/posts/mcp-server-weibo-beginners-journey/</link><pubDate>Sun, 15 Jun 2025 13:12:22 +0000</pubDate><guid>https://blog.yuanpei.me/posts/mcp-server-weibo-beginners-journey/</guid><description>去年，国外的 “毒舌” AI 应用 Twitter Personality 火爆一时。受其启发，博主跟风开发了一款类似的产品——微博性格报告，其核心功能是借助提示工程，从多个维度分析用户画像。从推特到微博，“数字世界的万花筒，每个社交平台都映照出你灵魂的不同切面”。我心目中的 AI，或许更像一面镜子，正试图从不同视角去观察人类的言行举止。然而，好景不长，后来微博上陆续出现了类似 “评论罗伯特”、“怼怼模拟器” 这类账号，因其学习成本更低、互动性更强，我的产品最终败北。再后来，该产品的代码变成了我 Agent 中的插件，而随着 MCP 协议的持续爆火，我终于将其以一个 MCP 服务器的形式再次推出，而这便是我今天想和大家分享的项目：mcp-server-weibo，这是一个基于 Model Context Protocol 的服务器，可以让大模型获取用户、微博、话题、评论等信息。
微博性格报告 @孙燕姿的微博性格报告 核心功能 作为一款针对微博的 MCP 服务器，目前 mcp-server-weibo 提供了下面 7 个工具：
名称 描述 备注 search_users 搜索微博用户 使用关键词搜索，可设置 limit 参数 get_profile 获取用户信息 使用用户唯一标识 uid get_feeds 获取用户动态 使用用户唯一标识 uid，可设置 limit 参数 get_trendings 获取微博热搜 可设置 limit 参数 search_content 搜索微博内容 使用关键词搜索，可设置 limit 和 page 参数 search_topics 搜索微博话题 使用关键词搜索，可设置 limit 和 page 参数 get_comments 获取指定微博下的评论 使用动态唯一标识 feed_id, 可设置 page 参数 使用方法 mcp-server-weibo 支持 stdio 和 streamable-http，可在以下支持 MCP 协议的客户端中使用：VS Code、Cursor、Windsurf、Cherry Studio、ChatWise、Claude Desktop 等等。使用方法如下：</description></item><item><title>AI 时代：聊聊大数据中的 MapReduce</title><link>https://blog.yuanpei.me/posts/2911923212/</link><pubDate>Fri, 19 Jan 2018 00:45:08 +0000</pubDate><guid>https://blog.yuanpei.me/posts/2911923212/</guid><description>各位朋友，大家好，我是 Payne，欢迎大家关注我的博客。最近读一本并行计算相关的书籍，在这本书中作者提到了 MapReduce。相信熟悉大数据领域的朋友，一定都知道 MapReduce 是 Hadoop 框架中重要的组成部分。在这篇文章中，博主将以函数式编程作为切入点，来和大家聊一聊大数据中的 MapReduce。如今人工智能正成为行业内竞相追逐的热点，选择 MapReduce 这个主题，更多的是希望带领大家一窥人工智能的门庭，更多深入的话题需要大家来探索和挖掘。
MapReduce 的前世今生 MapReduce 最早是由 Google 公司研究并提出的一种面向大规模数据处理的并行计算模型和方法。2003 年和 2004 年，Google 公司先后在国际会议上发表了关于 Google 分布式文件系统(GFS)和 MapReduce 的论文。这两篇论文公布了 Google 的 GFS 和 MapReduce 的基本原理和主要设计思想，我们通常所说的 Google 的三驾马车，实际上就是在说 GFS、BigTable 和 MapReduce。因此，这些论文的问世直接催生了 Hadoop 的诞生，可以说今天主流的大数据框架如 Hadoop、Spark 等，无一不是受到 Google 这些论文的影响，而这正是 MapReduce 由来，其得名则是因为函数式编程中的两个内置函数: map()和 reduce()。
我们常常说，脱离了业务场景去讨论一项技术是无意义的，这个原则在 MapReduce 上同样适用。众所周知，Google 是一家搜索引擎公司，其设计 MapReduce 的初衷，主要是为了解决搜索引擎中大规模网页数据的并行化处理。所以，我们可以说，MapReduce 其实是起源自 Web 检索的。而我们知道，Web 检索可以分为两部分，即获取网页内容并建立索引、根据网页索引来处理查询关键字。我们可以认为互联网上的每个网页都是一个文档，而每个文档中都会有不同的关键字，Google 会针对每一个关键字建立映射关系，即哪些文档中含有当前关键字，这是建立索引的过程。在建立索引以后，查询就会变得简单，因为现在我们可以按图索骥。
互联网诞生至今，网站及网页的数量越来越庞大，像 Google 这样的搜索引擎巨头是如何保证能够对 Web 上的内容进行检索的呢？答案是采用并行计算(Parallel)。硬件技术的不断革新，让计算机可以发挥多核的优势来处理数据，可当数据量庞大到单机无法处理的程度，就迫使我们不得不采用多台计算机进行并行计算。我们知道并行计算的思想是，将大数据分割成较小的数据块，交由不同的任务单元来处理，然后再将这些结果聚合起来。因此，可以将 MapReduce 理解为一种可以处理海量数据、运行在大规模集群上、具备高度容错能力、以并行处理方式执行的软件框架。MapReduce 是分治思想在大规模机器集群时代的集中体现(如图所示)，其中，Mapper 负责任务的划分，Reducer 负责结果的汇总。
MapReduce原理图 MapReduce 的推出给大数据并行处理带来了巨大的革命性影响，使其成为事实上的大数据处理的工业标准，是目前为止最为成功、最广为接受和最易于使用的大数据并行处理技术。广为人知的大数据框架 Hadoop，正是受到 MapReduce 的启发。自问世来，成为 Apache 基金会下最重要的项目，受到全球学术界和工业界的普遍关注，并得到推广和普及应用。MapReduce 的非凡意义在于，它提出了一个基于集群的高性能并行计算模型，允许使用市场上普通的商用服务器构成一个含有数十、数百甚至数千个节点的分布式并行计算集群，可以在集群节点上自动分配和执行任务以及收集计算结果，通过 Mapper 和 Reducer 提供了抽象的大数据处理并行编程接口，可以帮助开发人员更便捷地完成大规模数据处理的编程和计算工作。今天，Google 有超过 10000 个不同的项目已采用 MapReduce 来实现。</description></item></channel></rss>